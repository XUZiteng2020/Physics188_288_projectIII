{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5e9d982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Full dataset loaded from train.csv ===\n",
      "X shape: (21263, 81)\n",
      "y shape: (21263,)\n",
      "First 5 feature names: ['number_of_elements', 'mean_atomic_mass', 'wtd_mean_atomic_mass', 'gmean_atomic_mass', 'wtd_gmean_atomic_mass']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Please download all necessary packages below in a new conda environment:\n",
    "conda install numpy pandas scikit-learn\n",
    "conda install matplotlib jupyterlab\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 0. Load local CSV: 'train.csv'\n",
    "#    Assumes last column is 'critical_temp' or a column with that name.\n",
    "# ============================================================\n",
    "data_path = \"train.csv\"  # <- change to your actual path if needed\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# If the target column is not named 'critical_temp', adjust here:\n",
    "if \"critical_temp\" in df.columns:\n",
    "    target_col = \"critical_temp\"\n",
    "else:\n",
    "    # assume last column is the target\n",
    "    target_col = df.columns[-1]\n",
    "    print(f\"WARNING: 'critical_temp' not found; using last column '{target_col}' as target.\")\n",
    "\n",
    "X = df.drop(columns=[target_col])\n",
    "y = df[target_col]\n",
    "\n",
    "print(\"=== Full dataset loaded from train.csv ===\")\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)\n",
    "print(\"First 5 feature names:\", list(X.columns[:5]))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b68ea805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Split summary (70/15/15) ===\n",
      "Total samples:       21263\n",
      "Train size:          14883  (0.700)\n",
      "Validation size:     3190    (0.150)\n",
      "Verification size:   3190  (0.150)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 1: temp (85%) + verification (15%)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_temp, X_verif, y_temp, y_verif = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.15,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Step 2: train + val from temp\n",
    "val_fraction_within_temp = 0.15 / 0.85  # ≈ 0.17647 => overall 15% for val\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp,\n",
    "    y_temp,\n",
    "    test_size=val_fraction_within_temp,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "n_total = X.shape[0]\n",
    "print(\"=== Split summary (70/15/15) ===\")\n",
    "print(f\"Total samples:       {n_total}\")\n",
    "print(f\"Train size:          {X_train.shape[0]}  ({X_train.shape[0]/n_total:.3f})\")\n",
    "print(f\"Validation size:     {X_val.shape[0]}    ({X_val.shape[0]/n_total:.3f})\")\n",
    "print(f\"Verification size:   {X_verif.shape[0]}  ({X_verif.shape[0]/n_total:.3f})\")\n",
    "print()\n",
    "\n",
    "\n",
    "feat_names = X_train.columns  # all feature names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58274256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Helper: map feature name -> human-readable English description\n",
    "# ============================================================\n",
    "def describe_feature(name: str) -> str:\n",
    "    # Special case\n",
    "    if name == \"number_of_elements\":\n",
    "        return \"Number of distinct chemical elements present in the compound.\"\n",
    "\n",
    "    # Map property token to a human-readable phrase\n",
    "    prop_map = {\n",
    "        \"atomic_mass\": \"atomic mass of constituent elements\",\n",
    "        \"fie\": \"first ionization energy of constituent elements\",\n",
    "        \"atomic_radius\": \"atomic radius of constituent elements\",\n",
    "        \"Density\": \"mass density of constituent elements\",\n",
    "        \"ElectronAffinity\": \"electron affinity of constituent elements\",\n",
    "        \"FusionHeat\": \"heat of fusion (fusion enthalpy) of constituent elements\",\n",
    "        \"ThermalConductivity\": \"thermal conductivity of constituent elements\",\n",
    "        \"Valence\": \"valence electron count of constituent elements\",\n",
    "    }\n",
    "\n",
    "    # Identify prefix and property name\n",
    "    prefix = None\n",
    "    prop_token = None\n",
    "    for pre in [\n",
    "        \"wtd_mean_\", \"mean_\",\n",
    "        \"wtd_gmean_\", \"gmean_\",\n",
    "        \"wtd_entropy_\", \"entropy_\",\n",
    "        \"wtd_range_\", \"range_\",\n",
    "        \"wtd_std_\", \"std_\",\n",
    "    ]:\n",
    "        if name.startswith(pre):\n",
    "            prefix = pre\n",
    "            prop_token = name[len(pre):]\n",
    "            break\n",
    "\n",
    "    # Fallback: generic description\n",
    "    if prefix is None or prop_token is None:\n",
    "        return f\"Descriptor derived from elemental property '{name}', computed over the compound's composition.\"\n",
    "\n",
    "    prop_phrase = prop_map.get(\n",
    "        prop_token,\n",
    "        f\"property '{prop_token}' of constituent elements\"\n",
    "    )\n",
    "\n",
    "    # Construct description according to prefix\n",
    "    if prefix == \"mean_\":\n",
    "        return f\"Unweighted arithmetic mean of the {prop_phrase} in the compound.\"\n",
    "    if prefix == \"wtd_mean_\":\n",
    "        return f\"Atomic-fraction-weighted arithmetic mean of the {prop_phrase} in the compound.\"\n",
    "    if prefix == \"gmean_\":\n",
    "        return f\"Unweighted geometric mean of the {prop_phrase} in the compound.\"\n",
    "    if prefix == \"wtd_gmean_\":\n",
    "        return f\"Atomic-fraction-weighted geometric mean of the {prop_phrase} in the compound.\"\n",
    "    if prefix == \"entropy_\":\n",
    "        return f\"Entropy-like measure of how heterogeneous the {prop_phrase} are across different elements in the compound (higher = more diverse).\"\n",
    "    if prefix == \"wtd_entropy_\":\n",
    "        return f\"Atomic-fraction-weighted entropy-like measure of how heterogeneous the {prop_phrase} are across different elements in the compound.\"\n",
    "    if prefix == \"range_\":\n",
    "        return f\"Range (max minus min) of the {prop_phrase} among the elements in the compound.\"\n",
    "    if prefix == \"wtd_range_\":\n",
    "        return f\"Range-like measure of the {prop_phrase}, taking stoichiometric fractions into account.\"\n",
    "    if prefix == \"std_\":\n",
    "        return f\"Standard deviation of the {prop_phrase} among the elements in the compound.\"\n",
    "    if prefix == \"wtd_std_\":\n",
    "        return f\"Atomic-fraction-weighted standard deviation of the {prop_phrase} among the elements in the compound.\"\n",
    "\n",
    "    # Generic fallback\n",
    "    return f\"Descriptor derived from elemental property '{prop_token}', computed over the compound's composition.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9af81e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Correlation stats (train) ===\n",
      "Max |corr|: 0.7215652074556648\n",
      "Min |corr|: 0.027777584042315874\n",
      "Top-10 by |corr|:\n",
      " wtd_std_ThermalConductivity    0.721565\n",
      "range_ThermalConductivity      0.688322\n",
      "std_ThermalConductivity        0.654587\n",
      "range_atomic_radius            0.653350\n",
      "wtd_mean_Valence              -0.632144\n",
      "wtd_entropy_atomic_mass        0.626257\n",
      "wtd_gmean_Valence             -0.615472\n",
      "wtd_entropy_atomic_radius      0.602776\n",
      "mean_Valence                  -0.601244\n",
      "range_fie                      0.600165\n",
      "Name: critical_temp, dtype: float64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "\n",
    "\n",
    "# 2.1 Pearson correlation on TRAIN\n",
    "df_train = X_train.copy()\n",
    "df_train[\"critical_temp\"] = y_train\n",
    "\n",
    "corr_series = df_train.corr()[\"critical_temp\"].drop(\"critical_temp\")\n",
    "corr_sorted = corr_series.reindex(corr_series.abs().sort_values(ascending=False).index)\n",
    "\n",
    "print(\"=== Correlation stats (train) ===\")\n",
    "print(\"Max |corr|:\", corr_sorted.abs().max())\n",
    "print(\"Min |corr|:\", corr_sorted.abs().min())\n",
    "print(\"Top-10 by |corr|:\\n\", corr_sorted.head(10))\n",
    "print()\n",
    "\n",
    "top20_corr = set(corr_sorted.head(20).index)\n",
    "top40_corr = set(corr_sorted.head(40).index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "397e3973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Random Forest performance on validation set ===\n",
      "Val MSE:  108.5190\n",
      "Val RMSE: 10.4172\n",
      "Val R^2:  0.9080\n",
      "=== Random Forest feature_importances_ stats ===\n",
      "Min importance: 4.567328824738444e-05\n",
      "Max importance: 0.5403837984895478\n",
      "Number of zero importances: 0\n",
      "Number of non-zero importances: 81\n",
      "Sum of importances (should be 1.0): 1.0\n",
      "Top-10 features by RF importance:\n",
      "                             feature  rf_importance\n",
      "67        range_ThermalConductivity       0.540384\n",
      "64    wtd_gmean_ThermalConductivity       0.124821\n",
      "9                   std_atomic_mass       0.025296\n",
      "74                wtd_gmean_Valence       0.017075\n",
      "50         wtd_std_ElectronAffinity       0.015438\n",
      "72                 wtd_mean_Valence       0.011820\n",
      "31                     mean_Density       0.011071\n",
      "66  wtd_entropy_ThermalConductivity       0.009955\n",
      "43           gmean_ElectronAffinity       0.009735\n",
      "80                  wtd_std_Valence       0.009617\n",
      "\n",
      "Bottom-10 features by RF importance:\n",
      "                feature  rf_importance\n",
      "79         std_Valence       0.001039\n",
      "53    gmean_FusionHeat       0.001023\n",
      "1     mean_atomic_mass       0.000981\n",
      "75     entropy_Valence       0.000915\n",
      "73       gmean_Valence       0.000682\n",
      "37       range_Density       0.000537\n",
      "71        mean_Valence       0.000341\n",
      "57    range_FusionHeat       0.000299\n",
      "77       range_Valence       0.000220\n",
      "0   number_of_elements       0.000046\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2.2 Random Forest\n",
    "rf = RandomForestRegressor(\n",
    "    n_estimators=400,\n",
    "    max_depth=None,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "rf_importances = rf.feature_importances_\n",
    "\n",
    "# RF performance on VAL\n",
    "y_val_pred = rf.predict(X_val)\n",
    "mse_rf = mean_squared_error(y_val, y_val_pred)\n",
    "rmse_rf = mse_rf ** 0.5\n",
    "r2_rf = r2_score(y_val, y_val_pred)\n",
    "\n",
    "print(\"=== Random Forest performance on validation set ===\")\n",
    "print(f\"Val MSE:  {mse_rf:.4f}\")\n",
    "print(f\"Val RMSE: {rmse_rf:.4f}\")\n",
    "print(f\"Val R^2:  {r2_rf:.4f}\")\n",
    "\n",
    "# RF importance stats\n",
    "rf_min = rf_importances.min()\n",
    "rf_max = rf_importances.max()\n",
    "rf_zero_count = np.sum(rf_importances == 0.0)\n",
    "rf_nonzero_count = np.sum(rf_importances > 0.0)\n",
    "\n",
    "print(\"=== Random Forest feature_importances_ stats ===\")\n",
    "print(\"Min importance:\", rf_min)\n",
    "print(\"Max importance:\", rf_max)\n",
    "print(\"Number of zero importances:\", rf_zero_count)\n",
    "print(\"Number of non-zero importances:\", rf_nonzero_count)\n",
    "print(\"Sum of importances (should be 1.0):\", rf_importances.sum())\n",
    "\n",
    "rf_imp_df = (\n",
    "    pd.DataFrame({\"feature\": feat_names, \"rf_importance\": rf_importances})\n",
    "    .sort_values(\"rf_importance\", ascending=False)\n",
    ")\n",
    "\n",
    "print(\"Top-10 features by RF importance:\\n\", rf_imp_df.head(10))\n",
    "print(\"\\nBottom-10 features by RF importance:\\n\", rf_imp_df.tail(10))\n",
    "print()\n",
    "\n",
    "top20_rf = set(rf_imp_df[\"feature\"].head(20))\n",
    "top40_rf = set(rf_imp_df[\"feature\"].head(40))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a18a5d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Permutation importance (validation set) stats ===\n",
      "Shape of importances: (81, 10)\n",
      "Min perm_mean: 8.844370815708213e-06\n",
      "Max perm_mean: 0.6800750562121295\n",
      "Number of exactly-zero perm_mean: 0\n",
      "\n",
      "Top-10 features by permutation importance (mean):\n",
      "                           feature  perm_importance_mean  perm_importance_std\n",
      "67      range_ThermalConductivity              0.680075             0.024539\n",
      "64  wtd_gmean_ThermalConductivity              0.073488             0.004854\n",
      "9                 std_atomic_mass              0.049451             0.003305\n",
      "50       wtd_std_ElectronAffinity              0.027509             0.001424\n",
      "74              wtd_gmean_Valence              0.026928             0.000755\n",
      "31                   mean_Density              0.020090             0.001680\n",
      "72               wtd_mean_Valence              0.015679             0.000637\n",
      "55             entropy_FusionHeat              0.015575             0.000742\n",
      "35                entropy_Density              0.014068             0.001023\n",
      "27            range_atomic_radius              0.013668             0.000652\n",
      "\n",
      "Bottom-10 features by permutation importance (mean):\n",
      "                      feature  perm_importance_mean  perm_importance_std\n",
      "45  entropy_ElectronAffinity              0.000401             0.000113\n",
      "23       gmean_atomic_radius              0.000383             0.000097\n",
      "58      wtd_range_FusionHeat              0.000360             0.000092\n",
      "1           mean_atomic_mass              0.000258             0.000079\n",
      "77             range_Valence              0.000220             0.000039\n",
      "73             gmean_Valence              0.000125             0.000049\n",
      "37             range_Density              0.000115             0.000079\n",
      "71              mean_Valence              0.000068             0.000039\n",
      "57          range_FusionHeat              0.000057             0.000031\n",
      "0         number_of_elements              0.000009             0.000008\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2.3 Permutation importance on VAL\n",
    "perm_result = permutation_importance(\n",
    "    rf,\n",
    "    X_val,\n",
    "    y_val,\n",
    "    n_repeats=10,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "perm_means = perm_result.importances_mean\n",
    "perm_stds = perm_result.importances_std\n",
    "\n",
    "print(\"=== Permutation importance (validation set) stats ===\")\n",
    "print(\"Shape of importances:\", perm_result.importances.shape)\n",
    "print(\"Min perm_mean:\", perm_means.min())\n",
    "print(\"Max perm_mean:\", perm_means.max())\n",
    "print(\"Number of exactly-zero perm_mean:\", np.sum(perm_means == 0.0))\n",
    "print()\n",
    "\n",
    "perm_imp_df = (\n",
    "    pd.DataFrame({\n",
    "        \"feature\": feat_names,\n",
    "        \"perm_importance_mean\": perm_means,\n",
    "        \"perm_importance_std\": perm_stds,\n",
    "    })\n",
    "    .sort_values(\"perm_importance_mean\", ascending=False)\n",
    ")\n",
    "\n",
    "print(\"Top-10 features by permutation importance (mean):\\n\",\n",
    "      perm_imp_df.head(10))\n",
    "print(\"\\nBottom-10 features by permutation importance (mean):\\n\",\n",
    "      perm_imp_df.tail(10))\n",
    "print()\n",
    "\n",
    "top20_perm = set(perm_imp_df[\"feature\"].head(20))\n",
    "top40_perm = set(perm_imp_df[\"feature\"].head(40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84fd5c9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Summary table sanity check ===\n",
      "Summary_df shape: (81, 5)\n",
      "Any NaNs in rf_importance?: False\n",
      "Any NaNs in perm_importance_mean?: False\n",
      "Any NaNs in pearson_corr_with_Tc?: False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2.4 Merge three views into summary_df\n",
    "corr_df = corr_sorted.reset_index()\n",
    "corr_df.columns = [\"feature\", \"pearson_corr_with_Tc\"]\n",
    "\n",
    "summary_df = (\n",
    "    rf_imp_df\n",
    "    .merge(perm_imp_df, on=\"feature\", how=\"outer\")\n",
    "    .merge(corr_df, on=\"feature\", how=\"outer\")\n",
    ")\n",
    "\n",
    "print(\"=== Summary table sanity check ===\")\n",
    "print(\"Summary_df shape:\", summary_df.shape)\n",
    "print(\"Any NaNs in rf_importance?:\", summary_df[\"rf_importance\"].isna().any())\n",
    "print(\"Any NaNs in perm_importance_mean?:\", summary_df[\"perm_importance_mean\"].isna().any())\n",
    "print(\"Any NaNs in pearson_corr_with_Tc?:\", summary_df[\"pearson_corr_with_Tc\"].isna().any())\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8ca9b81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shortlist size (intersection of Top-40): 17\n",
      "Shortlist features:\n",
      " ['entropy_Density', 'entropy_FusionHeat', 'gmean_Density', 'range_ThermalConductivity', 'range_atomic_radius', 'range_fie', 'std_atomic_radius', 'wtd_entropy_Density', 'wtd_entropy_FusionHeat', 'wtd_entropy_Valence', 'wtd_entropy_atomic_mass', 'wtd_entropy_fie', 'wtd_gmean_Valence', 'wtd_mean_Valence', 'wtd_range_Valence', 'wtd_std_ThermalConductivity', 'wtd_std_atomic_radius']\n",
      "\n",
      "Saved shortlist CSV to: important_features_shortlist_superconductivity.csv\n"
     ]
    }
   ],
   "source": [
    "# 2.5 Shortlist = intersection of Top-40 from all three methods\n",
    "shortlist_features = sorted(list(top40_corr & top40_rf & top40_perm))\n",
    "print(f\"Shortlist size (intersection of Top-40): {len(shortlist_features)}\")\n",
    "print(\"Shortlist features:\\n\", shortlist_features)\n",
    "print()\n",
    "\n",
    "shortlist_df = summary_df[summary_df[\"feature\"].isin(shortlist_features)].copy()\n",
    "shortlist_df[\"description\"] = shortlist_df[\"feature\"].map(describe_feature)\n",
    "\n",
    "cols_order = [\n",
    "    \"feature\",\n",
    "    \"description\",\n",
    "    \"rf_importance\",\n",
    "    \"perm_importance_mean\",\n",
    "    \"perm_importance_std\",\n",
    "    \"pearson_corr_with_Tc\",\n",
    "]\n",
    "shortlist_df = shortlist_df[cols_order]\n",
    "\n",
    "shortlist_path = \"important_features_shortlist_superconductivity.csv\"\n",
    "shortlist_df.to_csv(shortlist_path, index=False)\n",
    "print(f\"Saved shortlist CSV to: {shortlist_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb5f1d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Longlist size (union of Top-20): 36\n",
      "Longlist features:\n",
      " ['entropy_Density', 'entropy_FusionHeat', 'entropy_Valence', 'entropy_atomic_radius', 'entropy_fie', 'gmean_Density', 'gmean_ElectronAffinity', 'gmean_Valence', 'mean_Density', 'mean_Valence', 'number_of_elements', 'range_ThermalConductivity', 'range_atomic_radius', 'range_fie', 'std_Density', 'std_ThermalConductivity', 'std_atomic_mass', 'std_atomic_radius', 'wtd_entropy_Density', 'wtd_entropy_FusionHeat', 'wtd_entropy_ThermalConductivity', 'wtd_entropy_Valence', 'wtd_entropy_atomic_mass', 'wtd_entropy_atomic_radius', 'wtd_gmean_ThermalConductivity', 'wtd_gmean_Valence', 'wtd_mean_ThermalConductivity', 'wtd_mean_Valence', 'wtd_mean_atomic_mass', 'wtd_range_Valence', 'wtd_range_atomic_mass', 'wtd_std_ElectronAffinity', 'wtd_std_ThermalConductivity', 'wtd_std_Valence', 'wtd_std_atomic_radius', 'wtd_std_fie']\n",
      "\n",
      "Saved longlist CSV to: important_features_longlist_superconductivity.csv\n"
     ]
    }
   ],
   "source": [
    "# 2.6 Longlist = union of Top-20 from all three methods\n",
    "longlist_features = sorted(list(top20_corr | top20_rf | top20_perm))\n",
    "print(f\"\\nLonglist size (union of Top-20): {len(longlist_features)}\")\n",
    "print(\"Longlist features:\\n\", longlist_features)\n",
    "print()\n",
    "\n",
    "longlist_df = summary_df[summary_df[\"feature\"].isin(longlist_features)].copy()\n",
    "longlist_df[\"description\"] = longlist_df[\"feature\"].map(describe_feature)\n",
    "longlist_df = longlist_df[cols_order]\n",
    "\n",
    "longlist_path = \"important_features_longlist_superconductivity.csv\"\n",
    "longlist_df.to_csv(longlist_path, index=False)\n",
    "print(f\"Saved longlist CSV to: {longlist_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2fd728f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== XGBoost (Hamidieh-style) on Train/Val split ===\n",
      "XGBoost Val MSE:  107.6924\n",
      "XGBoost Val RMSE: 10.3775 K\n",
      "XGBoost Val R^2:  0.9087\n",
      "Raw keys from booster.get_score(): ['number_of_elements', 'mean_atomic_mass', 'wtd_mean_atomic_mass', 'gmean_atomic_mass', 'wtd_gmean_atomic_mass', 'entropy_atomic_mass', 'wtd_entropy_atomic_mass', 'range_atomic_mass', 'wtd_range_atomic_mass', 'std_atomic_mass']\n",
      "\n",
      "Top 20 features by XGBoost gain importance:\n",
      "\n",
      "                              index  gain_importance\n",
      "0         range_ThermalConductivity    707066.937500\n",
      "1               range_atomic_radius     45775.847656\n",
      "2     wtd_gmean_ThermalConductivity     31960.435547\n",
      "3          wtd_std_ElectronAffinity     13223.087891\n",
      "4                   std_atomic_mass     11990.506836\n",
      "5            gmean_ElectronAffinity      8654.777344\n",
      "6      wtd_mean_ThermalConductivity      7767.352539\n",
      "7                 wtd_gmean_Valence      6302.254883\n",
      "8                         range_fie      5703.403320\n",
      "9       wtd_std_ThermalConductivity      5636.500977\n",
      "10                  wtd_std_Valence      5247.245605\n",
      "11             std_ElectronAffinity      5171.041016\n",
      "12                     mean_Density      5158.396973\n",
      "13                    gmean_Density      4661.006348\n",
      "14                 wtd_mean_Valence      4424.973145\n",
      "15  wtd_entropy_ThermalConductivity      4224.842773\n",
      "16                      std_Density      3925.760986\n",
      "17                  entropy_Density      3778.913086\n",
      "18      entropy_ThermalConductivity      2566.669189\n",
      "19                std_atomic_radius      2464.498535\n",
      "\n",
      "Saved XGBoost feature importance to: hamidieh_style_xgboost_feature_importance.csv\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 3. XGBoost feature importance (Hamidieh-style)\n",
    "#    Train on TRAIN, evaluate on VAL; verification is untouched.\n",
    "# ============================================================\n",
    "from xgboost import XGBRegressor\n",
    "print(\"\\n=== XGBoost (Hamidieh-style) on Train/Val split ===\")\n",
    "\n",
    "xgb = XGBRegressor(\n",
    "    n_estimators=500,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    objective=\"reg:squarederror\",\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on validation set\n",
    "y_val_pred_xgb = xgb.predict(X_val)\n",
    "mse_xgb = mean_squared_error(y_val, y_val_pred_xgb)\n",
    "rmse_xgb = mse_xgb ** 0.5\n",
    "r2_xgb = r2_score(y_val, y_val_pred_xgb)\n",
    "\n",
    "print(f\"XGBoost Val MSE:  {mse_xgb:.4f}\")\n",
    "print(f\"XGBoost Val RMSE: {rmse_xgb:.4f} K\")\n",
    "print(f\"XGBoost Val R^2:  {r2_xgb:.4f}\")\n",
    "\n",
    "# Extract gain-based feature importance\n",
    "booster = xgb.get_booster()\n",
    "gain_importance = booster.get_score(importance_type=\"gain\")\n",
    "\n",
    "print(\"Raw keys from booster.get_score():\", list(gain_importance.keys())[:10])\n",
    "\n",
    "imp_xgb = pd.DataFrame(\n",
    "    list(gain_importance.items()),\n",
    "    columns=[\"feature\", \"gain_importance\"]\n",
    ")\n",
    "\n",
    "# Ensure all features appear (unused features get 0)\n",
    "imp_xgb = (\n",
    "    imp_xgb\n",
    "    .set_index(\"feature\")\n",
    "    .reindex(feat_names, fill_value=0.0)\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "imp_xgb = imp_xgb.sort_values(\"gain_importance\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(\"\\nTop 20 features by XGBoost gain importance:\\n\")\n",
    "print(imp_xgb.head(20))\n",
    "\n",
    "ham_output_path = \"hamidieh_style_xgboost_feature_importance.csv\"\n",
    "imp_xgb.to_csv(ham_output_path, index=False)\n",
    "print(f\"\\nSaved XGBoost feature importance to: {ham_output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fde6e968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Column names check ===\n",
      "Shortlist columns: ['feature', 'description', 'rf_importance', 'perm_importance_mean', 'perm_importance_std', 'pearson_corr_with_Tc']\n",
      "Longlist  columns: ['feature', 'description', 'rf_importance', 'perm_importance_mean', 'perm_importance_std', 'pearson_corr_with_Tc']\n",
      "Hamidieh-style columns: ['index', 'gain_importance']\n",
      "\n",
      "WARNING: 'feature' not found in columns ['index', 'gain_importance']. Using first column 'index' as feature column.\n",
      "=== Size summary ===\n",
      "Number of features in shortlist: 17\n",
      "Number of features in longlist: 36\n",
      "Number of features in Hamidieh-style Top-16: 16\n",
      "Number of features in Hamidieh-style Top-37: 37\n",
      "\n",
      "Features that appear in BOTH shortlist and Hamidieh-style Top-16:\n",
      "['gmean_Density', 'range_ThermalConductivity', 'range_atomic_radius', 'range_fie', 'wtd_gmean_Valence', 'wtd_mean_Valence', 'wtd_std_ThermalConductivity']\n",
      "Count: 7\n",
      "\n",
      "Intersection between longlist and Hamidieh-style Top-37:\n",
      "['entropy_Density', 'entropy_FusionHeat', 'gmean_Density', 'gmean_ElectronAffinity', 'mean_Density', 'range_ThermalConductivity', 'range_atomic_radius', 'range_fie', 'std_Density', 'std_atomic_mass', 'std_atomic_radius', 'wtd_entropy_Density', 'wtd_entropy_FusionHeat', 'wtd_entropy_ThermalConductivity', 'wtd_gmean_ThermalConductivity', 'wtd_gmean_Valence', 'wtd_mean_ThermalConductivity', 'wtd_mean_Valence', 'wtd_range_Valence', 'wtd_range_atomic_mass', 'wtd_std_ElectronAffinity', 'wtd_std_ThermalConductivity', 'wtd_std_Valence']\n",
      "Intersection count: 23\n",
      "Overlap rate vs Hamidieh-style Top-37: 0.622\n",
      "Overlap rate vs longlist: 0.639\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1. Load the three CSV files\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "shortlist_path = \"important_features_shortlist_superconductivity.csv\"\n",
    "longlist_path = \"important_features_longlist_superconductivity.csv\"\n",
    "ham_path = \"hamidieh_style_xgboost_feature_importance.csv\"\n",
    "\n",
    "shortlist_df = pd.read_csv(shortlist_path)\n",
    "longlist_df = pd.read_csv(longlist_path)\n",
    "ham_df = pd.read_csv(ham_path)\n",
    "\n",
    "print(\"=== Column names check ===\")\n",
    "print(\"Shortlist columns:\", shortlist_df.columns.tolist())\n",
    "print(\"Longlist  columns:\", longlist_df.columns.tolist())\n",
    "print(\"Hamidieh-style columns:\", ham_df.columns.tolist())\n",
    "print()\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2. Robustly determine the 'feature' column name\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "def get_feature_col_name(df, preferred=\"feature\"):\n",
    "    \"\"\"Return the name of the feature column in df.\n",
    "    If 'preferred' exists, use it; otherwise use the first column.\"\"\"\n",
    "    cols = df.columns.tolist()\n",
    "    if preferred in cols:\n",
    "        return preferred\n",
    "    else:\n",
    "        print(f\"WARNING: '{preferred}' not found in columns {cols}. \"\n",
    "              f\"Using first column '{cols[0]}' as feature column.\")\n",
    "        return cols[0]\n",
    "\n",
    "shortlist_feat_col = get_feature_col_name(shortlist_df, preferred=\"feature\")\n",
    "longlist_feat_col = get_feature_col_name(longlist_df, preferred=\"feature\")\n",
    "ham_feat_col = get_feature_col_name(ham_df, preferred=\"feature\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3. Build feature sets\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "shortlist_features = set(shortlist_df[shortlist_feat_col].tolist())\n",
    "longlist_features = set(longlist_df[longlist_feat_col].tolist())\n",
    "\n",
    "# For Hamidieh-style table, we also need the importance column\n",
    "# to sort by. Try 'gain_importance' first, otherwise use the\n",
    "# second column as a fallback.\n",
    "if \"gain_importance\" in ham_df.columns:\n",
    "    imp_col = \"gain_importance\"\n",
    "else:\n",
    "    # Fallback: use second column as importance\n",
    "    if len(ham_df.columns) >= 2:\n",
    "        imp_col = ham_df.columns[1]\n",
    "        print(f\"WARNING: 'gain_importance' not found. Using column '{imp_col}' \"\n",
    "              \"as importance measure.\")\n",
    "    else:\n",
    "        raise ValueError(\"Hamidieh-style CSV does not have enough columns.\")\n",
    "\n",
    "ham_df_sorted = ham_df.sort_values(imp_col, ascending=False)\n",
    "\n",
    "# Take Top-16 and Top-37 features from Hamidieh-style ranking\n",
    "top16_ham_features = set(ham_df_sorted[ham_feat_col].head(16).tolist())\n",
    "top37_ham_features = set(ham_df_sorted[ham_feat_col].head(37).tolist())\n",
    "\n",
    "print(\"=== Size summary ===\")\n",
    "print(\"Number of features in shortlist:\", len(shortlist_features))\n",
    "print(\"Number of features in longlist:\", len(longlist_features))\n",
    "print(\"Number of features in Hamidieh-style Top-16:\", len(top16_ham_features))\n",
    "print(\"Number of features in Hamidieh-style Top-37:\", len(top37_ham_features))\n",
    "print()\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4. Intersection: shortlist ∩ Hamidieh-style Top-16\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "shortlist_ham16_intersection = shortlist_features & top16_ham_features\n",
    "\n",
    "print(\"Features that appear in BOTH shortlist and Hamidieh-style Top-16:\")\n",
    "print(sorted(list(shortlist_ham16_intersection)))\n",
    "print(\"Count:\", len(shortlist_ham16_intersection))\n",
    "print()\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 5. Overlap rate between longlist and Hamidieh-style Top-37\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "longlist_ham37_intersection = longlist_features & top37_ham_features\n",
    "\n",
    "intersect_count = len(longlist_ham37_intersection)\n",
    "top37_count = len(top37_ham_features)\n",
    "longlist_count = len(longlist_features)\n",
    "\n",
    "overlap_rate_vs_top37 = intersect_count / top37_count if top37_count > 0 else 0.0\n",
    "overlap_rate_vs_longlist = intersect_count / longlist_count if longlist_count > 0 else 0.0\n",
    "\n",
    "print(\"Intersection between longlist and Hamidieh-style Top-37:\")\n",
    "print(sorted(list(longlist_ham37_intersection)))\n",
    "print(\"Intersection count:\", intersect_count)\n",
    "print(f\"Overlap rate vs Hamidieh-style Top-37: {overlap_rate_vs_top37:.3f}\")\n",
    "print(f\"Overlap rate vs longlist: {overlap_rate_vs_longlist:.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "physics188",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
